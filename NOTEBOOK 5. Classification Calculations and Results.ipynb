{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f379970c",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a36765e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# math and dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Pipeline and Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# jupyter notebook full-width display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# pandas formatting\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbcb793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10M = pd.read_pickle('df_10M_clustered.pickle')\n",
    "X_all = pd.read_pickle('X_clustered.pickle')\n",
    "X_all = X_all.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed65dd3",
   "metadata": {},
   "source": [
    "# Setup inputs for statistical scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065df1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns for datasets\n",
    "\n",
    "y_column = 'in_B100'\n",
    "X_columns = [\n",
    "    'mode', 'acousticness', 'danceability', 'duration_ms', 'energy',\n",
    "    'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence'\n",
    "]\n",
    "genre_columns = [\n",
    "    'is_Adult_Standard', 'is_Rock', 'is_R&B', 'is_Country', 'is_Pop',\n",
    "    'is_Rap', 'is_Alternative', 'is_EDM', 'is_Metal'\n",
    "]\n",
    "cluster_columns = ['cluster', 'cluster2']\n",
    "other_columns = ['key', 'time_signature', 'genre', 'release_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18cfdba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "160\n",
      "210\n",
      "1800\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "param_by_model = {}\n",
    "\n",
    "params_lr = {}\n",
    "orders_of_magnitude = []\n",
    "for lst in [[int(x)/10000 for x in range(1, 11)],\n",
    "            [int(x)/1000 for x in range(1, 11)],\n",
    "            [int(x)/100 for x in range(1, 11)],\n",
    "            [int(x)/10 for x in range(1, 11)],\n",
    "            [1 * x for x in range(1, 11)],\n",
    "            [10 * x for x in range(1, 11)],\n",
    "            [100 * x for x in range(1, 11)],\n",
    "            [1000 * x for x in range(1, 11)]]:\n",
    "    orders_of_magnitude += lst\n",
    "params_lr['logisticregression__penalty'] = ['l1', 'l2']\n",
    "params_lr['logisticregression__C'] = orders_of_magnitude\n",
    "params_lr['logisticregression__solver'] = ['liblinear']\n",
    "param_by_model[0] = params_lr\n",
    "\n",
    "params_dt = {}\n",
    "params_dt['decisiontreeclassifier__max_depth'] = [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40, 50, 100, None]\n",
    "params_dt['decisiontreeclassifier__min_samples_leaf'] = [5, 10, 50, 100, 1000]\n",
    "params_dt['decisiontreeclassifier__criterion'] = ['gini', 'entropy']\n",
    "param_by_model[1] = params_dt\n",
    "\n",
    "params_knn = {}\n",
    "params_knn['kneighborsclassifier__n_neighbors'] = [x for x in range(2,20)]+[x for x in range(20,101,5)]\n",
    "params_knn['kneighborsclassifier__weights'] = ['uniform', 'distance']\n",
    "params_knn['kneighborsclassifier__metric'] = ['minkowski', 'euclidean', 'manhattan']\n",
    "param_by_model[2] = params_knn\n",
    "\n",
    "params_rf = {}\n",
    "params_rf['randomforestclassifier__n_estimators'] = [5, 10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "params_rf['randomforestclassifier__max_features'] = ['sqrt', 'log2']\n",
    "params_rf['randomforestclassifier__max_depth'] = [3, 5, 7, 10, 15, 20, 30, 50, 100, None]\n",
    "params_rf['randomforestclassifier__min_samples_leaf'] = [5, 10, 50, 100, 1000]\n",
    "params_rf['randomforestclassifier__bootstrap'] = [True, False]\n",
    "param_by_model[3] = params_rf\n",
    "\n",
    "params_ab = {}\n",
    "params_ab['adaboostclassifier__n_estimators'] = [10, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "params_ab['adaboostclassifier__learning_rate'] = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "params_ab['adaboostclassifier__algorithm'] = ['SAMME', 'SAMME.R']\n",
    "param_by_model[4] = params_ab\n",
    "\n",
    "# scoring metrics\n",
    "\n",
    "metrics = [\n",
    "    'balanced_accuracy', 'average_precision', 'neg_brier_score', 'f1', 'f1_micro', \n",
    "    'f1_macro', 'f1_weighted', 'neg_log_loss', 'precision', 'recall', 'roc_auc', 'jaccard'\n",
    "]\n",
    "\n",
    "# how many hyperparameter scenarios in the grid search\n",
    "\n",
    "def how_many_scenarios(n_ML):\n",
    "    n_scenarios = 1\n",
    "    for key in param_by_model[n_ML].keys():\n",
    "        n_scenarios *=  len(param_by_model[n_ML][key])\n",
    "    return n_scenarios\n",
    "\n",
    "for i in range(5):\n",
    "    print(how_many_scenarios(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5f89b",
   "metadata": {},
   "source": [
    "# Make Predictions Dataframe for Statistics\n",
    "* split into 5 stratified folds\n",
    "    * using a consistent random_state to use the same folds between tests\n",
    "* for each fold:\n",
    "    * train on undersampled training fold\n",
    "    * predict on full test fold\n",
    "    * add out of fold predictions to the predictions dataframe\n",
    "    \n",
    "NOTES: \n",
    "* Tuning individual models on limited datasets has been investigated in NOTEBOOK 5B (and 5D).\n",
    "* Random undersampling and oversampling were investigated in NOTEBOOK 5A\n",
    "    * More involved oversampling methods like SMOTE were not considered because the nature of music. For example, interpolating between modes leads to an atonal, non-musical result. Discrete combinations of features are likely to be important in terms of audio features as well. More importantly, with over 20k positive cases in our dataset, we should have enough data for a well trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb1834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "\n",
    "# use the same stratified split for all test cases\n",
    "stratified_5fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# scenarios to test\n",
    "stats_scenarios = [\n",
    "    'y_lr', 'y_dt', 'y_knn', 'y_rf', 'y_ab', 'y_lr_tuned', 'y_dt_tuned', 'y_knn_tuned', 'y_rf_tuned', 'y_ab_tuned', 'y_cl_1', 'y_cl_2', 'y_genres'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04281361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the dataframe\n",
    "df_predictions = pd.DataFrame(columns=stats_scenarios)\n",
    "df_predictions['y_actual'] = pd.NA  # for debugging\n",
    "df_predictions = pd.concat([X_all, df_predictions], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170000a",
   "metadata": {},
   "source": [
    "### Predict Using Default Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60bccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OUTDATED FUNCTION, UPDATED BELOW\n",
    "\n",
    "def evaluate_default_performance(dataframe, kfold, feature_columns, class_column, n_scenario):\n",
    "    \n",
    "    # use a copy of the dataframe to leave the original alone\n",
    "    dataframe = dataframe.copy()\n",
    "        \n",
    "    # entire dataset for predictions\n",
    "    X_, y_ = dataframe[feature_columns], dataframe[class_column]\n",
    "    \n",
    "    # initialise actual y and predicted y as blank dataframes\n",
    "    y_actual = pd.DataFrame()\n",
    "    y_pred = pd.DataFrame()\n",
    "\n",
    "    # loop through folds\n",
    "    for train_i, test_i in kfold.split(X_, y_):\n",
    "\n",
    "        # train test split for current fold\n",
    "        train_X, test_X = X_.iloc[train_i], X_.iloc[test_i]\n",
    "        train_y, test_y = y_.iloc[train_i], y_.iloc[test_i]\n",
    "        \n",
    "        # create and fit pipeline\n",
    "        undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "        \n",
    "        if n_scenario in [1, 6]:\n",
    "            model = DecisionTreeClassifier()\n",
    "        elif n_scenario in [2, 7]:\n",
    "            model = KNeighborsClassifier()\n",
    "        elif n_scenario in [3, 8]:\n",
    "            model = RandomForestClassifier()\n",
    "        elif n_scenario in [4, 9]:\n",
    "            model = AdaBoostClassifier()\n",
    "        else:\n",
    "            model = LogisticRegression()\n",
    "        \n",
    "        pipe = make_pipeline(undersampler, model)\n",
    "        \n",
    "        pipe.fit(train_X, train_y)\n",
    "        \n",
    "        # append results\n",
    "        y_pred_temp = pipe.predict(test_X)\n",
    "        y_pred_temp = pd.concat([\n",
    "            pd.DataFrame(y_pred_temp),\n",
    "            pd.DataFrame(test_i, columns=[''])\n",
    "        ], axis=1).set_index('')\n",
    "        y_pred = pd.concat([y_pred, y_pred_temp], axis=0)\n",
    "        y_actual = pd.concat([y_actual, test_y], axis=0)  # for debugging\n",
    "\n",
    "    # return the full sorted results, appended into the input dataframe\n",
    "    dataframe[stats_scenarios[n_scenario]] = y_pred.sort_index()\n",
    "    dataframe['y_actual'] = y_actual.sort_index()  # for debugging, this should always be equal to in_B100 or folds are misaligned\n",
    "    \n",
    "    # DEBUGGING: should be zero\n",
    "    is_ERRORS = sum(dataframe['in_B100'] != dataframe['y_actual'])\n",
    "    if is_ERRORS != 0:\n",
    "        print('THERE WERE ERRORS!!!! (compare the in_B100 and y_actual columns)')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6312b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Logistic Regression\n",
    "n_scenario = 0\n",
    "df_predictions = evaluate_default_performance(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e57ea621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Decision Tree\n",
    "n_scenario = 1\n",
    "df_predictions = evaluate_default_performance(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356d78e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# K Nearest Neighbours\n",
    "n_scenario = 2\n",
    "df_predictions = evaluate_default_performance(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f2f1b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random Forest\n",
    "n_scenario = 3\n",
    "df_predictions = evaluate_default_performance(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c0aa6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# AdaBoost\n",
    "n_scenario = 4\n",
    "df_predictions = evaluate_default_performance(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84f7f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df_predictions.to_pickle('df_predictions_DEFAULT.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa724041",
   "metadata": {},
   "source": [
    "### Make Predictions With Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6159b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OUTDATED FUNCTION, UPDATED BELOW\n",
    "\n",
    "def append_predictions(dataframe, kfold, feature_columns, class_column, n_scenario):\n",
    "    \n",
    "    # use a copy of the dataframe to leave the original alone\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # entire dataset for predictions\n",
    "    X_, y_ = dataframe[feature_columns], dataframe[class_column]\n",
    "    \n",
    "    # initialise actual y and predicted y as blank dataframes\n",
    "    y_actual = pd.DataFrame()\n",
    "    y_pred = pd.DataFrame()\n",
    "\n",
    "    # loop through folds\n",
    "    for train_i, test_i in kfold.split(X_, y_):\n",
    "\n",
    "        # train test split for current fold\n",
    "        train_X, test_X = X_.iloc[train_i], X_.iloc[test_i]\n",
    "        train_y, test_y = y_.iloc[train_i], y_.iloc[test_i]\n",
    "        \n",
    "        # create pipeline\n",
    "        undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "        \n",
    "        if n_scenario in [1, 6]:\n",
    "            n_ML = 1\n",
    "            model = DecisionTreeClassifier()\n",
    "        elif n_scenario in [2, 7]:\n",
    "            n_ML = 2\n",
    "            model = KNeighborsClassifier()\n",
    "        elif n_scenario in [3, 8]:\n",
    "            n_ML = 3\n",
    "            model = RandomForestClassifier()\n",
    "        elif n_scenario in [4, 9]:\n",
    "            n_ML = 4\n",
    "            model = AdaBoostClassifier()\n",
    "        else:\n",
    "            n_ML = 0\n",
    "            model = LogisticRegression()\n",
    "        \n",
    "        pipe = make_pipeline(undersampler, model)\n",
    "        \n",
    "        # tune hyperparameters if required\n",
    "        if n_scenario in [5, 6, 7, 8, 9]:\n",
    "            # create and fit gridsearch\n",
    "            grid = GridSearchCV(\n",
    "                pipe,\n",
    "                param_grid = param_by_model[n_ML],\n",
    "                scoring = 'roc_auc'\n",
    "            )\n",
    "            grid.fit(train_X, train_y)\n",
    "            y_pred_temp = grid.predict(test_X)\n",
    "        else:\n",
    "            pipe.fit(train_X, train_y)\n",
    "            y_pred_temp = pipe.predict(test_X)\n",
    "        \n",
    "        # append results\n",
    "        y_pred_temp = pd.concat([\n",
    "            pd.DataFrame(y_pred_temp),\n",
    "            pd.DataFrame(test_i, columns=[''])\n",
    "        ], axis=1).set_index('')\n",
    "        y_pred = pd.concat([y_pred, y_pred_temp], axis=0)\n",
    "        y_actual = pd.concat([y_actual, test_y], axis=0)  # for debugging\n",
    "\n",
    "    # return the full sorted results, appended into the input dataframe\n",
    "    dataframe[stats_scenarios[n_scenario]] = y_pred.sort_index()\n",
    "    dataframe['y_actual'] = y_actual.sort_index()  # for debugging, this should always be equal to in_B100 or folds are misaligned\n",
    "    \n",
    "    # DEBUGGING: should be zero\n",
    "    is_ERRORS = sum(dataframe['in_B100'] != dataframe['y_actual'])\n",
    "    if is_ERRORS != 0:\n",
    "        print('THERE WERE ERRORS!!!! (compare the in_B100 and y_actual columns)')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "08d88ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61.333333333333336, 66.66666666666667, 3780, 5580.0, 213.3)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is wrong, they take a lot longer (esp adaboost)\n",
    "\n",
    "n_LogisticRegression = 160\n",
    "n_DecisionTreeClassifier = 160\n",
    "n_KNeighborsClassifier = 210\n",
    "n_RandomForestClassifier = 1800\n",
    "n_AdaBoostClassifier = 162\n",
    "\n",
    "# KNN and Random Forest Will Take Too Long, only try \n",
    "n_LogisticRegression * 23/60, n_DecisionTreeClassifier * 25/60, n_KNeighborsClassifier * 18, n_RandomForestClassifier  * 186/60, n_AdaBoostClassifier * 79/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "84af98f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 54min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_scenario = 5\n",
    "df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)\n",
    "\n",
    "# save dataframe\n",
    "df_predictions.to_pickle('df_predictions_TUNED.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7a8037c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 56min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_scenario = 6\n",
    "df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)\n",
    "\n",
    "# save dataframe\n",
    "df_predictions.to_pickle('df_predictions_TUNED.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357104a7",
   "metadata": {},
   "source": [
    "### These are too time consuming to complete\n",
    "these were investigated using smaller fitting dataset in earlier notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # calculated this from early afternoon until the next morning, and it didn't complete\n",
    "# # drop adaboost from partially tuned models\n",
    "# n_scenario = 9\n",
    "# df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)\n",
    "\n",
    "# save dataframe\n",
    "# df_predictions.to_pickle('df_predictions_FULLYTUNED.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # this should take too long\n",
    "# n_scenario = 7\n",
    "# df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)\n",
    "\n",
    "# save dataframe\n",
    "# df_predictions.to_pickle('df_predictions_FULLYTUNED.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # this should take too long\n",
    "# n_scenario = 8\n",
    "# df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)\n",
    "\n",
    "# save dataframe\n",
    "# df_predictions.to_pickle('df_predictions_FULLYTUNED.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32129b",
   "metadata": {},
   "source": [
    "### Make predictions separating into clusters and genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "752ccb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS IS THE FINAL APPEND PREDICITONS FUNCTION\n",
    "# should be applicable to all situations, but haven't been tested for all scenarios (some take hours/days to run)\n",
    "# works on scenario 0, 5, 6, 10+\n",
    "\n",
    "def append_predictions(dataframe, kfold, feature_columns, class_column, n_scenario):\n",
    "    \"\"\"\n",
    "    loop through folds, append out of fold predictions to database of predictions\n",
    "    cluster could be 'genre', 'cluster1', 'cluster2', or False (default)\n",
    "    \"\"\"\n",
    "    # use a copy of the dataframe to leave the original alone\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # based on the scenario number, do we need to cluster?\n",
    "    if n_scenario == 10:\n",
    "        cluster = 'cluster1'\n",
    "    elif n_scenario == 11:\n",
    "        cluster = 'cluster2'\n",
    "    elif n_scenario == 12:\n",
    "        cluster = 'genre'\n",
    "    else:\n",
    "        cluster = False\n",
    "    \n",
    "    # entire dataset for predictions\n",
    "    X_, y_ = dataframe[feature_columns], dataframe[class_column]\n",
    "    \n",
    "    # initialise actual y and predicted y as blank dataframes\n",
    "    y_actual = pd.DataFrame()\n",
    "    y_pred = pd.DataFrame()\n",
    "\n",
    "    # loop through folds\n",
    "    for train_i, test_i in kfold.split(X_, y_):\n",
    "\n",
    "        # train test split for current fold\n",
    "        train_X, test_X = X_.iloc[train_i], X_.iloc[test_i]\n",
    "        train_y, test_y = y_.iloc[train_i], y_.iloc[test_i]\n",
    "        \n",
    "        # create pipeline\n",
    "        undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "        \n",
    "        # initialise a new classifier (inside fold loop to prevent spillover from refitting)\n",
    "        if n_scenario in [1, 6]:\n",
    "            n_ML = 1\n",
    "            model = DecisionTreeClassifier()\n",
    "        elif n_scenario in [2, 7]:\n",
    "            n_ML = 2\n",
    "            model = KNeighborsClassifier()\n",
    "        elif n_scenario in [3, 8]:\n",
    "            n_ML = 3\n",
    "            model = RandomForestClassifier()\n",
    "        elif n_scenario in [4, 9]:\n",
    "            n_ML = 4\n",
    "            model = AdaBoostClassifier()\n",
    "        else:\n",
    "            n_ML = 0\n",
    "            model = LogisticRegression()\n",
    "        \n",
    "        pipe = make_pipeline(undersampler, model)\n",
    "        \n",
    "        # THREE OPTIONS: tune hyperparameters, loop through clusters, just fit the pipe\n",
    "        \n",
    "        # OPTION 1: tune hyperparameters, tune/fit the grid\n",
    "        if n_scenario in [5, 6, 7, 8, 9]:\n",
    "            # create and fit gridsearch\n",
    "            # NOTE: n_jobs=1 leads to a PicklingError\n",
    "            \"\"\"\n",
    "            from: https://stackoverflow.com/questions/56884020/spacy-with-joblib-library-generates-pickle-picklingerror-could-not-pickle-the\n",
    "                'Same issue. I solved by changing the backend from loky to threading in Parallel.'\n",
    "            \"\"\"\n",
    "            grid = GridSearchCV(\n",
    "                pipe,\n",
    "                param_grid = param_by_model[n_ML],\n",
    "                scoring = 'roc_auc'\n",
    "            )\n",
    "            grid.fit(train_X, train_y)\n",
    "            y_pred_temp = grid.predict(test_X)\n",
    "            \n",
    "        # OPTION 2: loop through clusters, individually fit the pipe\n",
    "        elif cluster:  \n",
    "            \n",
    "            # initialise dataframe to append results\n",
    "            y_pred_temp = pd.DataFrame()\n",
    "            \n",
    "            # cluster 1\n",
    "            if cluster == 'cluster1':\n",
    "                for i in range(4):\n",
    "                    # this seems convoluted, but it speeds the code 10x vs loc bool combo\n",
    "                    cluster_index = dataframe[['cluster']][dataframe['cluster'] == i]\n",
    "                    i_train_cluster = cluster_index[cluster_index.index.isin(train_i)].index\n",
    "                    i_test_cluster = cluster_index[cluster_index.index.isin(test_i)].index\n",
    "                    \n",
    "                    # iloc doesn't work on index, it works on position, loc works\n",
    "                    train_X_c = train_X.loc[i_train_cluster]\n",
    "                    test_X_c = test_X.loc[i_test_cluster]\n",
    "                    train_y_c = train_y.loc[i_train_cluster]\n",
    "                    test_y_c = test_y.loc[i_test_cluster]\n",
    "                    \n",
    "                    # make prediction\n",
    "                    pipe.fit(train_X_c, train_y_c)\n",
    "                    y_pred_cluster = pipe.predict(test_X_c)\n",
    "                    \n",
    "                    # append prediction to y_pred_temp\n",
    "                    y_pred_temp = pd.concat([\n",
    "                        y_pred_temp,\n",
    "                        pd.DataFrame(y_pred_cluster, index=i_test_cluster)\n",
    "                    ], axis=0)\n",
    "                    \n",
    "            # cluster 2\n",
    "            elif cluster == 'cluster2':\n",
    "                \n",
    "                for i in range(10):\n",
    "                    # this seems convoluted, but it speeds the code up by a factor of 10\n",
    "                    cluster_index = dataframe[['cluster2']][dataframe['cluster2'] == i]\n",
    "                    i_train_cluster = cluster_index[cluster_index.index.isin(train_i)].index\n",
    "                    i_test_cluster = cluster_index[cluster_index.index.isin(test_i)].index\n",
    "                    \n",
    "                    # iloc doesn't work on index, it works on position, loc works\n",
    "                    train_X_c = train_X.loc[i_train_cluster]\n",
    "                    test_X_c = test_X.loc[i_test_cluster]\n",
    "                    train_y_c = train_y.loc[i_train_cluster]\n",
    "                    test_y_c = test_y.loc[i_test_cluster]\n",
    "                    \n",
    "                    # make prediction\n",
    "                    pipe.fit(train_X_c, train_y_c)\n",
    "                    y_pred_cluster = pipe.predict(test_X_c)\n",
    "                    \n",
    "                    # append prediction to y_pred_temp\n",
    "                    y_pred_temp = pd.concat([\n",
    "                        y_pred_temp,\n",
    "                        pd.DataFrame(y_pred_cluster, index=i_test_cluster)\n",
    "                    ], axis=0)\n",
    "      \n",
    "            # genre\n",
    "            elif cluster == 'genre':\n",
    "                genre_columns = [\n",
    "                    'is_Adult_Standard', 'is_Rock', 'is_R&B', 'is_Country', 'is_Pop',\n",
    "                    'is_Rap', 'is_Alternative', 'is_EDM', 'is_Metal'\n",
    "                ]\n",
    "                for genre in genre_columns:\n",
    "                    \n",
    "                    # NOTE: could consider adding a 'misc' genre, which is not in these genres\n",
    "                    \n",
    "                    # this seems convoluted, but it speeds the code 10x vs loc bool combo\n",
    "                    cluster_index = dataframe[[genre]][dataframe[genre]]  # confirm that this works\n",
    "                    i_train_cluster = cluster_index[cluster_index.index.isin(train_i)].index\n",
    "                    i_test_cluster = cluster_index[cluster_index.index.isin(test_i)].index\n",
    "                    \n",
    "                    # iloc doesn't work on index, it works on position, loc works\n",
    "                    train_X_c = train_X.loc[i_train_cluster]\n",
    "                    test_X_c = test_X.loc[i_test_cluster]\n",
    "                    train_y_c = train_y.loc[i_train_cluster]\n",
    "                    test_y_c = test_y.loc[i_test_cluster]\n",
    "                    \n",
    "                    # make prediction\n",
    "                    pipe.fit(train_X_c, train_y_c)\n",
    "                    y_pred_cluster = pipe.predict(test_X_c)\n",
    "                    \n",
    "                    # append prediction to y_pred_temp\n",
    "                    y_pred_temp = pd.concat([\n",
    "                        y_pred_temp,\n",
    "                        pd.DataFrame(y_pred_cluster, index=i_test_cluster)\n",
    "                    ], axis=0)\n",
    "                    \n",
    "            # no matching cluster exists\n",
    "            else:\n",
    "                print('NO SUCH CLUSTER')  # could raise an error instead\n",
    "                return dataframe  # do nothing, just return the input dataframe\n",
    "            \n",
    "            # sort y_pred_temp by index so it aligns properly\n",
    "            y_pred_temp = np.array(y_pred_temp.sort_index())\n",
    "                \n",
    "        # OPTION 3: just fit the pipe\n",
    "        else:  \n",
    "            pipe.fit(train_X, train_y)\n",
    "            y_pred_temp = pipe.predict(test_X)\n",
    "        \n",
    "        # debugging statement: after fitting the data for the fold\n",
    "        print('fold complete')\n",
    "                \n",
    "        # fitting complete for fold\n",
    "        # append results\n",
    "        y_pred_temp = pd.concat([\n",
    "            pd.DataFrame(y_pred_temp),\n",
    "            pd.DataFrame(test_i, columns=[''])\n",
    "        ], axis=1).set_index('')\n",
    "        y_pred = pd.concat([y_pred, y_pred_temp], axis=0)\n",
    "        y_actual = pd.concat([y_actual, test_y], axis=0)  # for debugging\n",
    "        \n",
    "    # return the full sorted results, appended into the input dataframe\n",
    "    dataframe[stats_scenarios[n_scenario]] = y_pred.sort_index()\n",
    "    dataframe['y_actual'] = y_actual.sort_index()  # for debugging, this should always be equal to in_B100 or folds are misaligned\n",
    "    \n",
    "    # DEBUGGING: should be zero\n",
    "    is_ERRORS = sum(dataframe['in_B100'] != dataframe['y_actual'])\n",
    "    if is_ERRORS != 0:\n",
    "        print('THERE WERE ERRORS!!!! (compare the in_B100 and y_actual columns)')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e373eaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold complete\n",
      "fold complete\n",
      "fold complete\n",
      "fold complete\n",
      "fold complete\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# clustering results: cluster 1\n",
    "df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario=10)\n",
    "\n",
    "# save dataframe\n",
    "df_predictions.to_pickle('df_predictions_CLUSTERS.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ae507cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold complete\n",
      "fold complete\n",
      "fold complete\n",
      "fold complete\n",
      "fold complete\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# clustering results: cluster 2\n",
    "df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario=11)\n",
    "\n",
    "# save dataframe\n",
    "df_predictions.to_pickle('df_predictions_CLUSTERS.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "770cfb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold complete\n",
      "fold complete\n",
      "fold complete\n",
      "fold complete\n",
      "fold complete\n",
      "Wall time: 52.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# clustering results: genres\n",
    "df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario=12)\n",
    "\n",
    "# save dataframe\n",
    "df_predictions.to_pickle('df_predictions_CLUSTERS.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd58ad",
   "metadata": {},
   "source": [
    "# Save Final Predictions Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dedc9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final predictions dataframe\n",
    "df_predictions.drop(['y_knn_tuned', 'y_rf_tuned', 'y_ab_tuned', 'y_actual'], axis=1).to_pickle('df_predictions.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13463036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
