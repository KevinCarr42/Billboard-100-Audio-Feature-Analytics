{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# math and dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Pipeline and Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# Undersampling \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# jupyter notebook full-width display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# pandas formatting\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10M = pd.read_pickle('df_10M_clustered.pickle')\n",
    "X_all = pd.read_pickle('X_clustered.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_column = 'in_B100'\n",
    "X_columns = [\n",
    "    'mode', 'acousticness', 'danceability', 'duration_ms', 'energy',\n",
    "    'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence'\n",
    "]\n",
    "genre_columns = [\n",
    "    'is_Adult_Standard', 'is_Rock', 'is_R&B', 'is_Country', 'is_Pop',\n",
    "    'is_Rap', 'is_Alternative', 'is_EDM', 'is_Metal'\n",
    "]\n",
    "cluster_columns = ['cluster', 'cluster2']\n",
    "other_columns = ['key', 'time_signature', 'genre', 'release_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb9282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict with all 'name': (X, y) key match pairs\n",
    "clusters = {}\n",
    "\n",
    "# entire predictive dataset\n",
    "clusters['All'] = (X_all[X_columns+genre_columns], X_all[y_column])\n",
    "# clusters['All'] = (X_all[X_columns], X_all[y_column])\n",
    "\n",
    "# add genres\n",
    "for genre in genre_columns:\n",
    "    title = genre[3:]\n",
    "    clusters[title] = (X_all[X_all[genre]][X_columns], X_all[X_all[genre]][y_column])\n",
    "    \n",
    "# add clusters\n",
    "for n in sorted(X_all['cluster'].unique()):\n",
    "    title = genre[3:]\n",
    "    clusters['cluster1_' + str(n)] = (X_all[X_all['cluster'] == n][X_columns], X_all[X_all['cluster'] == n][y_column])\n",
    "    \n",
    "for n in sorted(X_all['cluster2'].unique()):\n",
    "    title = genre[3:]\n",
    "    clusters['cluster2_' + str(n)] = (X_all[X_all['cluster2'] == n][X_columns], X_all[X_all['cluster2'] == n][y_column])\n",
    "    \n",
    "# setup tuning algorithm with a small dataset\n",
    "small = X_all.sample(10_000, random_state=42)\n",
    "X_small = small[X_columns]\n",
    "y_small = small[y_column]\n",
    "clusters['small'] = (X_small, y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ca9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios to check\n",
    "\n",
    "metrics = [\n",
    "    'balanced_accuracy', 'average_precision', 'neg_brier_score', 'f1', 'f1_micro', \n",
    "    'f1_macro', 'f1_weighted', 'neg_log_loss', 'precision', 'recall', 'roc_auc', 'jaccard'\n",
    "]\n",
    "\n",
    "cluster1_keys = [\n",
    "    'cluster1_0', 'cluster1_1', 'cluster1_2', 'cluster1_3'\n",
    "]\n",
    "\n",
    "cluster2_keys = [\n",
    "    'cluster2_0', 'cluster2_1', 'cluster2_2', 'cluster2_3', 'cluster2_4', \n",
    "    'cluster2_5', 'cluster2_6', 'cluster2_7', 'cluster2_8', 'cluster2_9',\n",
    "]\n",
    "\n",
    "genre_keys = [\n",
    "    'Adult_Standard', 'Rock', 'R&B', 'Country', 'Pop', 'Rap', 'Alternative', 'EDM', 'Metal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the neural network\n",
    "\n",
    "# start with width of number of features \n",
    "# (could encode to higher dimensions)\n",
    "tf_width = len(X_columns)\n",
    "tf_dropout = 0\n",
    "\n",
    "# setup\n",
    "tf_model = Sequential(name='Sequential')\n",
    "\n",
    "# hidden layers\n",
    "how_many_hidden_layers = 5\n",
    "for i in range(how_many_hidden_layers):\n",
    "    tf_model.add(Dense(tf_width, activation=\"relu\", name=f'Dense_{i}'))\n",
    "    tf_model.add(Dropout(tf_dropout, name=f'Dropout_{i}'))\n",
    "\n",
    "# output\n",
    "tf_model.add(Dense(1, activation='sigmoid', name='Output'))\n",
    "\n",
    "# setup a checkpoint to save model\n",
    "checkpoint = ModelCheckpoint('tf_model', save_best_only=True)\n",
    "\n",
    "# compile model\n",
    "tf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87656517",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train the model\n",
    "\n",
    "# pick the dataset\n",
    "am_testing = True\n",
    "if am_testing:\n",
    "    dataset = 'small'\n",
    "else:\n",
    "    dataset = 'cluster2_7' # good amount of hits, about 1M songs \n",
    "X_, y_ = clusters[dataset]\n",
    "\n",
    "# split the dataset into train test, then separate validation set for fitting the neural network\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=42, stratify=y_)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42, stratify=y_train)\n",
    "\n",
    "# undersample data\n",
    "undersample = True\n",
    "if undersample:\n",
    "    undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "    X_train, y_train = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# fit the data\n",
    "tf_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, callbacks=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843539a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdfe9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
