{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2104a521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# math and dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Pipeline and Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# Undersampling \n",
    "# Note: undersampling was used in at least 1 paper predicting popularity (Gao 2021)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# jupyter notebook full-width display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# pandas formatting\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "438a02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10M = pd.read_pickle('df_10M_clustered.pickle')\n",
    "X_all = pd.read_pickle('X_clustered.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efc2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_column = 'in_B100'\n",
    "X_columns = [\n",
    "    'mode', 'acousticness', 'danceability', 'duration_ms', 'energy',\n",
    "    'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence'\n",
    "]\n",
    "genre_columns = [\n",
    "    'is_Adult_Standard', 'is_Rock', 'is_R&B', 'is_Country', 'is_Pop',\n",
    "    'is_Rap', 'is_Alternative', 'is_EDM', 'is_Metal'\n",
    "]\n",
    "cluster_columns = ['cluster', 'cluster2']\n",
    "other_columns = ['key', 'time_signature', 'genre', 'release_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c35a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict with all 'name': (X, y) key match pairs\n",
    "clusters = {}\n",
    "\n",
    "# entire predictive dataset\n",
    "clusters['All'] = (X_all[X_columns+genre_columns], X_all[y_column])\n",
    "# clusters['All'] = (X_all[X_columns], X_all[y_column])\n",
    "\n",
    "# add genres\n",
    "for genre in genre_columns:\n",
    "    title = genre[3:]\n",
    "    clusters[title] = (X_all[X_all[genre]][X_columns], X_all[X_all[genre]][y_column])\n",
    "    \n",
    "# add clusters\n",
    "for n in sorted(X_all['cluster'].unique()):\n",
    "    title = genre[3:]\n",
    "    clusters['cluster1_' + str(n)] = (X_all[X_all['cluster'] == n][X_columns], X_all[X_all['cluster'] == n][y_column])\n",
    "    \n",
    "for n in sorted(X_all['cluster2'].unique()):\n",
    "    title = genre[3:]\n",
    "    clusters['cluster2_' + str(n)] = (X_all[X_all['cluster2'] == n][X_columns], X_all[X_all['cluster2'] == n][y_column])\n",
    "    \n",
    "# setup tuning algorithm with a small dataset\n",
    "small = X_all.sample(10_000, random_state=42)\n",
    "X_small = small[X_columns]\n",
    "y_small = small[y_column]\n",
    "clusters['small'] = (X_small, y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eb307d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios to check\n",
    "\n",
    "metrics = [\n",
    "    'balanced_accuracy', 'average_precision', 'neg_brier_score', 'f1', 'f1_micro', \n",
    "    'f1_macro', 'f1_weighted', 'neg_log_loss', 'precision', 'recall', 'roc_auc', 'jaccard'\n",
    "]\n",
    "\n",
    "cluster1_keys = [\n",
    "    'cluster1_0', 'cluster1_1', 'cluster1_2', 'cluster1_3'\n",
    "]\n",
    "\n",
    "cluster2_keys = [\n",
    "    'cluster2_0', 'cluster2_1', 'cluster2_2', 'cluster2_3', 'cluster2_4', \n",
    "    'cluster2_5', 'cluster2_6', 'cluster2_7', 'cluster2_8', 'cluster2_9',\n",
    "]\n",
    "\n",
    "genre_keys = [\n",
    "    'Adult_Standard', 'Rock', 'R&B', 'Country', 'Pop', 'Rap', 'Alternative', 'EDM', 'Metal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the neural network\n",
    "\n",
    "# start with width of number of features \n",
    "# (could encode to higher dimensions)\n",
    "tf_width = len(X_columns)\n",
    "tf_dropout = 0\n",
    "\n",
    "# setup\n",
    "tf_model = Sequential(name='Sequential')\n",
    "\n",
    "# hidden layers\n",
    "how_many_hidden_layers = 5\n",
    "\n",
    "for i in range(how_many_hidden_layers):\n",
    "tf_model.add(Dense(tf_width, activation=\"relu\", name=f'Dense_{i}'))\n",
    "tf_model.add(Dropout(tf_dropout, name=f'Dropout_{i}'))\n",
    "\n",
    "# output\n",
    "tf_model.add(Dense(1, activation='sigmoid', name='Output'))\n",
    "\n",
    "# compile model\n",
    "tf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12224d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train the model\n",
    "\n",
    "# pick the dataset\n",
    "dataset = 'small'\n",
    "X_, y_ = clusters[dataset]\n",
    "\n",
    "# split the dataset into train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.3, random_state=42, stratify=y_)\n",
    "\n",
    "# undersample data\n",
    "undersample = True\n",
    "if undersample:\n",
    "    undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "    X_train, y_train = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# fit the data\n",
    "tf_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=40, callbacks=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5a3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b9f46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the dataset\n",
    "dataset = 'cluster1_0'\n",
    "X_, y_ = clusters[dataset]\n",
    "\n",
    "# split the dataset into train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.3, random_state=42, stratify=y_)\n",
    "\n",
    "# undersample data\n",
    "undersample = True\n",
    "if undersample:\n",
    "    undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "    X_train, y_train = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6461b79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, (258,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum(), y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ebb9192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370086,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9ed767b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21229"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters['All'][1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c85355e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 21229 8827719 0.0024048114807460456\n",
      "Adult_Standard 3678 208860 0.01760988221775352\n",
      "Rock 6311 657177 0.009603196703475625\n",
      "R&B 2965 136578 0.02170920646077699\n",
      "Country 2448 270216 0.009059419131361577\n",
      "Pop 3904 540126 0.00722794310957073\n",
      "Rap 2084 415998 0.005009639469420526\n",
      "Alternative 177 84950 0.002083578575632725\n",
      "EDM 103 249597 0.0004126652163287219\n",
      "Metal 142 245399 0.0005786494647492451\n",
      "cluster1_0 184 1233618 0.00014915476265748393\n",
      "cluster1_1 14659 3978190 0.0036848415988175527\n",
      "cluster1_2 5844 2304781 0.002535598826960132\n",
      "cluster1_3 542 1311130 0.00041338387497807233\n",
      "cluster2_0 2156 995561 0.0021656131568030487\n",
      "cluster2_1 3443 1086979 0.003167494496213818\n",
      "cluster2_2 56 819425 6.834060469231473e-05\n",
      "cluster2_3 101 755361 0.00013371090114528021\n",
      "cluster2_4 128 414193 0.00030903467707083415\n",
      "cluster2_5 3772 934132 0.004037973220058835\n",
      "cluster2_6 883 317876 0.0027778127320087077\n",
      "cluster2_7 7848 1730621 0.0045347883794314295\n",
      "cluster2_8 2401 1217802 0.0019715848717607623\n",
      "cluster2_9 441 555769 0.0007934951391675318\n",
      "small 30 10000 0.003\n"
     ]
    }
   ],
   "source": [
    "# adult standard is just better because of randomness...\n",
    "for cluster in clusters:\n",
    "    print(cluster, clusters[cluster][1].sum(), clusters[cluster][1].count(), clusters[cluster][1].sum() / clusters[cluster][1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f2bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
