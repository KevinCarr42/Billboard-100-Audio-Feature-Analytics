{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f379970c",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a36765e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# math and dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# statistics\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Pipeline and Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# jupyter notebook full-width display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# pandas formatting\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cbcb793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10M = pd.read_pickle('df_10M_clustered.pickle')\n",
    "X_all = pd.read_pickle('X_clustered.pickle')\n",
    "X_all.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed65dd3",
   "metadata": {},
   "source": [
    "# Setup inputs for statistical scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "065df1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns for datasets\n",
    "\n",
    "y_column = 'in_B100'\n",
    "X_columns = [\n",
    "    'mode', 'acousticness', 'danceability', 'duration_ms', 'energy',\n",
    "    'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence'\n",
    "]\n",
    "genre_columns = [\n",
    "    'is_Adult_Standard', 'is_Rock', 'is_R&B', 'is_Country', 'is_Pop',\n",
    "    'is_Rap', 'is_Alternative', 'is_EDM', 'is_Metal'\n",
    "]\n",
    "cluster_columns = ['cluster', 'cluster2']\n",
    "other_columns = ['key', 'time_signature', 'genre', 'release_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18cfdba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 160\n",
      "DecisionTreeClassifier 160\n",
      "KNeighborsClassifier 210\n",
      "RandomForestClassifier 1800\n",
      "AdaBoostClassifier 162\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "param_by_model = {}\n",
    "\n",
    "params_lr = {}\n",
    "orders_of_magnitude = []\n",
    "for lst in [[int(x)/10000 for x in range(1, 11)],\n",
    "            [int(x)/1000 for x in range(1, 11)],\n",
    "            [int(x)/100 for x in range(1, 11)],\n",
    "            [int(x)/10 for x in range(1, 11)],\n",
    "            [1 * x for x in range(1, 11)],\n",
    "            [10 * x for x in range(1, 11)],\n",
    "            [100 * x for x in range(1, 11)],\n",
    "            [1000 * x for x in range(1, 11)]]:\n",
    "    orders_of_magnitude += lst\n",
    "params_lr['logisticregression__penalty'] = ['l1', 'l2']\n",
    "params_lr['logisticregression__C'] = orders_of_magnitude\n",
    "params_lr['logisticregression__solver'] = ['liblinear']\n",
    "param_by_model[0] = params_lr\n",
    "\n",
    "params_dt = {}\n",
    "params_dt['decisiontreeclassifier__max_depth'] = [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40, 50, 100, None]\n",
    "params_dt['decisiontreeclassifier__min_samples_leaf'] = [5, 10, 50, 100, 1000]\n",
    "params_dt['decisiontreeclassifier__criterion'] = ['gini', 'entropy']\n",
    "param_by_model[1] = params_dt\n",
    "\n",
    "params_knn = {}\n",
    "params_knn['kneighborsclassifier__n_neighbors'] = [x for x in range(2,20)]+[x for x in range(20,101,5)]\n",
    "params_knn['kneighborsclassifier__weights'] = ['uniform', 'distance']\n",
    "params_knn['kneighborsclassifier__metric'] = ['minkowski', 'euclidean', 'manhattan']\n",
    "param_by_model[2] = params_knn\n",
    "\n",
    "params_rf = {}\n",
    "params_rf['randomforestclassifier__n_estimators'] = [5, 10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "params_rf['randomforestclassifier__max_features'] = ['sqrt', 'log2']\n",
    "params_rf['randomforestclassifier__max_depth'] = [3, 5, 7, 10, 15, 20, 30, 50, 100, None]\n",
    "params_rf['randomforestclassifier__min_samples_leaf'] = [5, 10, 50, 100, 1000]\n",
    "params_rf['randomforestclassifier__bootstrap'] = [True, False]\n",
    "param_by_model[3] = params_rf\n",
    "\n",
    "params_ab = {}\n",
    "params_ab['adaboostclassifier__n_estimators'] = [10, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "params_ab['adaboostclassifier__learning_rate'] = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "params_ab['adaboostclassifier__algorithm'] = ['SAMME', 'SAMME.R']\n",
    "param_by_model[4] = params_ab\n",
    "\n",
    "# scoring metrics\n",
    "\n",
    "metrics = [\n",
    "    'balanced_accuracy', 'average_precision', 'neg_brier_score', 'f1', 'f1_micro', \n",
    "    'f1_macro', 'f1_weighted', 'neg_log_loss', 'precision', 'recall', 'roc_auc', 'jaccard'\n",
    "]\n",
    "\n",
    "# how many hyperparameter scenarios in the grid search\n",
    "\n",
    "def how_many_scenarios(n_ML):\n",
    "    n_scenarios = 1\n",
    "    for key in param_by_model[n_ML].keys():\n",
    "        n_scenarios *=  len(param_by_model[n_ML][key])\n",
    "    return n_scenarios\n",
    "\n",
    "for i in range(5):\n",
    "    print(str(ML_algorithms[i]())[:-2], how_many_scenarios(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90531be",
   "metadata": {},
   "source": [
    "# Make Predictions Dataframe for Statistics\n",
    "* split into 5 stratified folds\n",
    "    * using a consistent random_state to use the same folds between tests\n",
    "* for each fold:\n",
    "    * train on undersampled training fold\n",
    "    * predict on full test fold\n",
    "    * add out of fold predictions to add to predictions dataframe\n",
    "    \n",
    "NOTES: \n",
    "* Tuning individual models on limited datasets has been investigated in NOTEBOOK 5B (and 5D).\n",
    "* Random undersampling and oversampling were investigated in NOTEBOOK 5A\n",
    "    * More oversampling methods like SMOTE were not considered because the nature of music. For example, interpolating between modes leads to an atonal, non-musical result. Discrete combinations of features are likely to be important in terms of audio features as well. More importantly, with over 20k positive cases in our dataset, we should have enough data for a well trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "189bc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the dataframe\n",
    "\n",
    "# use the same stratified split for all test cases\n",
    "stratified_5fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# scenarios to test\n",
    "stats_scenarios = [\n",
    "    'y_lr', 'y_dt', 'y_knn', 'y_rf', 'y_ab', 'y_lr_tuned', 'y_dt_tuned', 'y_knn_tuned', 'y_rf_tuned', 'y_ab_tuned', 'y_cl_1', 'y_cl_2', 'y_genres'\n",
    "]\n",
    "\n",
    "df_predictions = pd.DataFrame(columns=stats_scenarios)\n",
    "df_predictions['y_actual'] = pd.NA  # for debugging\n",
    "df_predictions = pd.concat([X_all, df_predictions], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80457d5e",
   "metadata": {},
   "source": [
    "### Predict Using Default Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e0afd78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UPDATE THIS WITH FUTURE FUNCTION\n",
    "\n",
    "def evaluate_default_performance(dataframe, kfold, feature_columns, class_column, n_scenario):\n",
    "    \n",
    "    # entire dataset for predictions\n",
    "    X_, y_ = dataframe[feature_columns], dataframe[class_column]\n",
    "    \n",
    "    # initialise actual y and predicted y as blank dataframes\n",
    "    y_actual = pd.DataFrame()\n",
    "    y_pred = pd.DataFrame()\n",
    "\n",
    "    # loop through folds\n",
    "    for train_i, test_i in kfold.split(X_, y_):\n",
    "\n",
    "        # train test split for current fold\n",
    "        train_X, test_X = X_.iloc[train_i], X_.iloc[test_i]\n",
    "        train_y, test_y = y_.iloc[train_i], y_.iloc[test_i]\n",
    "        \n",
    "        # create and fit pipeline\n",
    "        undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "        \n",
    "        if n_scenario in [1, 6]:\n",
    "            model = DecisionTreeClassifier()\n",
    "        elif n_scenario in [2, 7]:\n",
    "            model = KNeighborsClassifier()\n",
    "        elif n_scenario in [3, 8]:\n",
    "            model = RandomForestClassifier()\n",
    "        elif n_scenario in [4, 9]:\n",
    "            model = AdaBoostClassifier()\n",
    "        else:\n",
    "            model = LogisticRegression()\n",
    "        \n",
    "        pipe = make_pipeline(undersampler, model)\n",
    "        \n",
    "        pipe.fit(train_X, train_y)\n",
    "        \n",
    "        # append results\n",
    "        y_pred_temp = pipe.predict(test_X)\n",
    "        y_pred_temp = pd.concat([\n",
    "            pd.DataFrame(y_pred_temp),\n",
    "            pd.DataFrame(test_i, columns=[''])\n",
    "        ], axis=1).set_index('')\n",
    "        y_pred = pd.concat([y_pred, y_pred_temp], axis=0)\n",
    "        y_actual = pd.concat([y_actual, test_y], axis=0)  # for debugging\n",
    "\n",
    "    # return the full sorted results, appended into the input dataframe\n",
    "    dataframe[stats_scenarios[n_scenario]] = y_pred.sort_index()\n",
    "    dataframe['y_actual'] = y_actual.sort_index()  # for debugging, this should always be equal to in_B100 or folds are misaligned\n",
    "    \n",
    "    # DEBUGGING: should be zero\n",
    "    is_ERRORS = sum(dataframe['in_B100'] != dataframe['y_actual'])\n",
    "    if is_ERRORS != 0:\n",
    "        print('THERE WERE ERRORS!!!! (compare the in_B100 and y_actual columns)')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7b2ebde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Logistic Regression\n",
    "n_scenario = 0\n",
    "df_predictions = evaluate_default_performance(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "00f591dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Decision Tree\n",
    "n_scenario = 1\n",
    "df_predictions = evaluate_default_performance(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fd078417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# K Nearest Neighbours\n",
    "n_scenario = 2\n",
    "df_predictions = evaluate_default_performance(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cc9c95ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random Forest\n",
    "n_scenario = 3\n",
    "df_predictions = evaluate_default_performance(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d18469f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# AdaBoost\n",
    "n_scenario = 4\n",
    "df_predictions = evaluate_default_performance(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "70cb34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df_predictions.to_pickle('df_predictions_DEFAULT.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c21247",
   "metadata": {},
   "source": [
    "### Make Predictions With Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c8d12681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_predictions(dataframe, kfold, feature_columns, class_column, n_scenario):\n",
    "    \n",
    "    # entire dataset for predictions\n",
    "    X_, y_ = dataframe[feature_columns], dataframe[class_column]\n",
    "    \n",
    "    # initialise actual y and predicted y as blank dataframes\n",
    "    y_actual = pd.DataFrame()\n",
    "    y_pred = pd.DataFrame()\n",
    "\n",
    "    # loop through folds\n",
    "    for train_i, test_i in kfold.split(X_, y_):\n",
    "\n",
    "        # train test split for current fold\n",
    "        train_X, test_X = X_.iloc[train_i], X_.iloc[test_i]\n",
    "        train_y, test_y = y_.iloc[train_i], y_.iloc[test_i]\n",
    "        \n",
    "        # create pipeline\n",
    "        undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "        \n",
    "        if n_scenario in [1, 6]:\n",
    "            n_ML = 1\n",
    "            model = DecisionTreeClassifier()\n",
    "        elif n_scenario in [2, 7]:\n",
    "            n_ML = 2\n",
    "            model = KNeighborsClassifier()\n",
    "        elif n_scenario in [3, 8]:\n",
    "            n_ML = 3\n",
    "            model = RandomForestClassifier()\n",
    "        elif n_scenario in [4, 9]:\n",
    "            n_ML = 4\n",
    "            model = AdaBoostClassifier()\n",
    "        else:\n",
    "            n_ML = 0\n",
    "            model = LogisticRegression()\n",
    "        \n",
    "        pipe = make_pipeline(undersampler, model)\n",
    "        \n",
    "        # tune hyperparameters if required\n",
    "        if n_scenario in [5, 6, 7, 8, 9]:\n",
    "            # create and fit gridsearch\n",
    "            grid = GridSearchCV(\n",
    "                pipe,\n",
    "                param_grid = param_by_model[n_ML]\n",
    "            )\n",
    "            grid.fit(train_X, train_y)\n",
    "            y_pred_temp = grid.predict(test_X)\n",
    "        else:\n",
    "            pipe.fit(train_X, train_y)\n",
    "            y_pred_temp = pipe.predict(test_X)\n",
    "        \n",
    "        # append results\n",
    "        y_pred_temp = pd.concat([\n",
    "            pd.DataFrame(y_pred_temp),\n",
    "            pd.DataFrame(test_i, columns=[''])\n",
    "        ], axis=1).set_index('')\n",
    "        y_pred = pd.concat([y_pred, y_pred_temp], axis=0)\n",
    "        y_actual = pd.concat([y_actual, test_y], axis=0)  # for debugging\n",
    "\n",
    "    # return the full sorted results, appended into the input dataframe\n",
    "    dataframe[stats_scenarios[n_scenario]] = y_pred.sort_index()\n",
    "    dataframe['y_actual'] = y_actual.sort_index()  # for debugging, this should always be equal to in_B100 or folds are misaligned\n",
    "    \n",
    "    # DEBUGGING: should be zero\n",
    "    is_ERRORS = sum(dataframe['in_B100'] != dataframe['y_actual'])\n",
    "    if is_ERRORS != 0:\n",
    "        print('THERE WERE ERRORS!!!! (compare the in_B100 and y_actual columns)')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "545690fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61.333333333333336, 66.66666666666667, 3780, 5580.0, 213.3)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_LogisticRegression = 160\n",
    "n_DecisionTreeClassifier = 160\n",
    "n_KNeighborsClassifier = 210\n",
    "n_RandomForestClassifier = 1800\n",
    "n_AdaBoostClassifier = 162\n",
    "\n",
    "# KNN and Random Forest Will Take Too Long, only try \n",
    "n_LogisticRegression * 23/60, n_DecisionTreeClassifier * 25/60, n_KNeighborsClassifier * 18, n_RandomForestClassifier  * 186/60, n_AdaBoostClassifier * 79/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d03aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_scenario = 5\n",
    "df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707fc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_scenario = 6\n",
    "df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_scenario = 9\n",
    "df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a23c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df_predictions.to_pickle('df_predictions_TUNED.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this should take too long\n",
    "n_scenario = 7\n",
    "df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88976711",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this should take too long\n",
    "n_scenario = 8\n",
    "df_predictions = append_predictions(df_predictions, stratified_5fold, X_columns, y_column, n_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb57b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df_predictions.to_pickle('df_predictions_FULLYTUNED.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40c271",
   "metadata": {},
   "source": [
    "### Make predictions separating into clusters and genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3343e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f3641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1ce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df_predictions.to_pickle('df_predictions_CLUSTERS.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7041d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final predictions dataframe\n",
    "df_predictions.to_pickle('df_predictions.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f852f2ee",
   "metadata": {},
   "source": [
    "# Explore Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friedman Test (Wilcoxn if required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms \n",
    "## NEED TO COMBINE WITH id to compare AF\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
