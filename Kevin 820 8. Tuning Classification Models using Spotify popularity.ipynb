{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f379970c",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a36765e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import spotipy\n",
    "\n",
    "# math and dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Pipeline and Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# Undersampling \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# jupyter notebook full-width display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# pandas formatting\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbcb793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10M = pd.read_pickle('df_10M_clustered.pickle')\n",
    "X_all = pd.read_pickle('X_clustered.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6c6f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# popularity from the sql database\n",
    "\"\"\"\n",
    "    SELECT id, popularity FROM tracks\n",
    "\"\"\"\n",
    "df_sql_popularity = pd.read_csv('popularity_by_track_sql.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91d862",
   "metadata": {},
   "source": [
    "# SKIP THIS STEP \n",
    "# (see note below)\n",
    "##### Get Popularity for Missing Tracks from B100\n",
    "\n",
    "NOTE:\n",
    "* this data will not lead to a consistent, apples-to-apples comparision:\n",
    "    * popularity is based on recency of the plays\n",
    "    * the SQL database and API 'popularity' have recency which are not in sync\n",
    "    * it is more accurate to use just the SQL database for this prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1305d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise popularity dataframe for B100 songs\n",
    "\n",
    "df_B100_songs = pd.read_pickle('df_B100_songs.pickle')\n",
    "\n",
    "df_B100_popularity = df_B100_songs.copy()\n",
    "df_B100_popularity['popularity'] = pd.NA\n",
    "df_B100_popularity = df_B100_popularity[['id', 'popularity']]\n",
    "\n",
    "# confirm no duplicates\n",
    "df_B100_popularity.id.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae69c6",
   "metadata": {},
   "source": [
    "##### get a temporary authorization token from: https://developer.spotify.com/console/get-search-item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the temporary token\n",
    "TEMP_TOKEN = input('Enter token: ')\n",
    "\n",
    "# create a spotify object\n",
    "spotify = spotipy.Spotify(auth=TEMP_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popularity(track_id):\n",
    "    track_info = spotify.track(track_id)\n",
    "    popularity = track_info['popularity']\n",
    "    \n",
    "    return popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c305b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# loop to GET popularity\n",
    "\n",
    "counter = 0\n",
    "start_over_at = 0\n",
    "if start_over_at == 0:\n",
    "    id_errors = set()\n",
    "\n",
    "for i, row in df_B100_popularity.iterrows():\n",
    "        \n",
    "    if counter % 100 == 0:\n",
    "        print(counter, end=' ')\n",
    "    if counter % 1000 == 0:\n",
    "        print()\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    if i < start_over_at:  # where we timed out last time\n",
    "        continue\n",
    "    \n",
    "    # save temp file\n",
    "    if counter % 1000 == 0:\n",
    "        df_B100_popularity.to_pickle('df_B100_popularity_TEMP.pickle')\n",
    "    \n",
    "    # does this track have a null popularity value? if not, next row\n",
    "    if not df_B100_popularity.iloc[[i]].isnull()['popularity'].values[0]:\n",
    "        continue    \n",
    "    \n",
    "    # current id for lookup in API\n",
    "    track_id = row.id\n",
    "    \n",
    "    # lookup song info from API and set the popularity value for that track\n",
    "    try:\n",
    "        df_B100_popularity.loc[i, 'popularity'] = get_popularity(track_id)\n",
    "    except:  # any error should \n",
    "        print(' -- get_popularity() didnt work -- ', track_id)\n",
    "        id_errors.add(track_id)\n",
    "        df_B100_popularity.loc[i, 'popularity'] = 0  # set it to zero anyway\n",
    "\n",
    "\n",
    "# save the dataframe\n",
    "df_B100_popularity.to_pickle('df_B100_popularity_COMPLETE.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bad701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many missing values\n",
    "len(id_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f248e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many null\n",
    "df_B100_popularity.popularity.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine df_sql_popularity and df_B100_popularity into df_popularity\n",
    "df_popularity = pd.concat([df_B100_popularity, df_sql_popularity]).reset_index(drop=True)\n",
    "df_popularity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm no duplicates\n",
    "df_popularity.duplicated(subset='id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b60a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "df_popularity.to_pickle('df_popularity.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea13ae1",
   "metadata": {},
   "source": [
    "##### Combine Spotify Popularity with X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf234f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99bdca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dd34da5",
   "metadata": {},
   "source": [
    "# START OVER HERE\n",
    "# Choose a Threshold for 'is_Popular'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa062d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>popularity</th>\n",
       "      <td>6.118</td>\n",
       "      <td>10.580</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean    std   min    1%   50%    99%     max\n",
       "popularity 6.118 10.580 0.000 0.000 1.000 48.000 100.000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql_popularity.describe([0.01, 0.99])['mean':'max'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74edd02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the top percentile is about popularity of 50\n",
    "# this seems like a good, arbitrary threshold\n",
    "\n",
    "popularity_threshold = 50\n",
    "\n",
    "df_sql_popularity['is_Popular'] = df_sql_popularity.popularity > popularity_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1e8c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with X_all\n",
    "X_all = pd.merge(X_all, df_sql_popularity, left_index=True, right_on='id').set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5084f7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8030875"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_all.artist.isnull().sum()\n",
    "X_all.is_Popular.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed65dd3",
   "metadata": {},
   "source": [
    "# Create Datasets for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "065df1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_column = 'is_Popular'\n",
    "X_columns = [\n",
    "    'mode', 'acousticness', 'danceability', 'duration_ms', 'energy',\n",
    "    'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence'\n",
    "]\n",
    "genre_columns = [\n",
    "    'is_Adult_Standard', 'is_Rock', 'is_R&B', 'is_Country', 'is_Pop',\n",
    "    'is_Rap', 'is_Alternative', 'is_EDM', 'is_Metal'\n",
    "]\n",
    "cluster_columns = ['cluster', 'cluster2']\n",
    "other_columns = ['key', 'time_signature', 'genre', 'release_date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d64aee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict with all 'name': (X, y) key match pairs\n",
    "clusters = {}\n",
    "\n",
    "# entire predictive dataset\n",
    "clusters['All'] = (X_all[X_columns+genre_columns], X_all[y_column])\n",
    "# clusters['All'] = (X_all[X_columns], X_all[y_column])\n",
    "\n",
    "# add genres\n",
    "for genre in genre_columns:\n",
    "    title = genre[3:]\n",
    "    clusters[title] = (X_all[X_all[genre]][X_columns], X_all[X_all[genre]][y_column])\n",
    "    \n",
    "# add clusters\n",
    "for n in sorted(X_all['cluster'].unique()):\n",
    "    title = genre[3:]\n",
    "    clusters['cluster1_' + str(n)] = (X_all[X_all['cluster'] == n][X_columns], X_all[X_all['cluster'] == n][y_column])\n",
    "    \n",
    "for n in sorted(X_all['cluster2'].unique()):\n",
    "    title = genre[3:]\n",
    "    clusters['cluster2_' + str(n)] = (X_all[X_all['cluster2'] == n][X_columns], X_all[X_all['cluster2'] == n][y_column])\n",
    "    \n",
    "# setup tuning algorithm with a small dataset\n",
    "small = X_all.sample(10_000, random_state=42)\n",
    "X_small = small[X_columns]\n",
    "y_small = small[y_column]\n",
    "clusters['small'] = (X_small, y_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98d9ab",
   "metadata": {},
   "source": [
    "# Tune Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a9f9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_keys = [\n",
    "    'All', \n",
    "    'Adult_Standard', 'Rock', 'R&B', 'Country', 'Pop', 'Rap', 'Alternative', 'EDM', 'Metal', \n",
    "    'cluster1_0', 'cluster1_1', 'cluster1_2', 'cluster1_3', \n",
    "    'cluster2_0', 'cluster2_1', 'cluster2_2', 'cluster2_3', 'cluster2_4', \n",
    "    'cluster2_5', 'cluster2_6', 'cluster2_7', 'cluster2_8', 'cluster2_9',\n",
    "    'small'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a2b45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model parameters for grid search\n",
    "\n",
    "ML_algorithms = [\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    KNeighborsClassifier,\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier\n",
    "]\n",
    "\n",
    "param_by_model = {}\n",
    "\n",
    "params_lr = {}\n",
    "orders_of_magnitude = []\n",
    "for lst in [[int(x)/10000 for x in range(1, 11)],\n",
    "            [int(x)/1000 for x in range(1, 11)],\n",
    "            [int(x)/100 for x in range(1, 11)],\n",
    "            [int(x)/10 for x in range(1, 11)],\n",
    "            [1 * x for x in range(1, 11)],\n",
    "            [10 * x for x in range(1, 11)],\n",
    "            [100 * x for x in range(1, 11)],\n",
    "            [1000 * x for x in range(1, 11)]]:\n",
    "    orders_of_magnitude += lst\n",
    "params_lr['logisticregression__penalty'] = ['l1', 'l2']\n",
    "params_lr['logisticregression__C'] = orders_of_magnitude\n",
    "params_lr['logisticregression__solver'] = ['liblinear']\n",
    "param_by_model[0] = params_lr\n",
    "\n",
    "params_dt = {}\n",
    "params_dt['decisiontreeclassifier__max_depth'] = [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40, 50, 100, None]\n",
    "params_dt['decisiontreeclassifier__min_samples_leaf'] = [5, 10, 50, 100, 1000]\n",
    "params_dt['decisiontreeclassifier__criterion'] = ['gini', 'entropy']\n",
    "param_by_model[1] = params_dt\n",
    "\n",
    "params_knn = {}\n",
    "params_knn['kneighborsclassifier__n_neighbors'] = [x for x in range(2,20)]+[x for x in range(20,101,5)]\n",
    "params_knn['kneighborsclassifier__weights'] = ['uniform', 'distance']\n",
    "params_knn['kneighborsclassifier__metric'] = ['minkowski', 'euclidean', 'manhattan']\n",
    "param_by_model[2] = params_knn\n",
    "\n",
    "params_rf = {}\n",
    "params_rf['randomforestclassifier__n_estimators'] = [5, 10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "params_rf['randomforestclassifier__max_features'] = ['sqrt', 'log2']\n",
    "params_rf['randomforestclassifier__max_depth'] = [3, 5, 7, 10, 15, 20, 30, 50, 100, None]\n",
    "params_rf['randomforestclassifier__min_samples_leaf'] = [5, 10, 50, 100, 1000]\n",
    "params_rf['randomforestclassifier__bootstrap'] = [True, False]\n",
    "param_by_model[3] = params_rf\n",
    "\n",
    "params_ab = {}\n",
    "params_ab['adaboostclassifier__n_estimators'] = [10, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "params_ab['adaboostclassifier__learning_rate'] = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "params_ab['adaboostclassifier__algorithm'] = ['SAMME', 'SAMME.R']\n",
    "param_by_model[4] = params_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e96e64ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 160\n",
      "DecisionTreeClassifier 160\n",
      "KNeighborsClassifier 210\n",
      "RandomForestClassifier 1800\n",
      "AdaBoostClassifier 162\n"
     ]
    }
   ],
   "source": [
    "# how many scenarios in the grid search\n",
    "\n",
    "def how_many_scenarios(n_ML):\n",
    "    n_scenarios = 1\n",
    "    for key in param_by_model[n_ML].keys():\n",
    "        n_scenarios *=  len(param_by_model[n_ML][key])\n",
    "    return n_scenarios\n",
    "\n",
    "for i in range(5):\n",
    "    print(str(ML_algorithms[i]())[:-2], how_many_scenarios(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55c5b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_metric_model(n_ML, dataset='small', n_cv=5, scoring='roc_auc', undersample=True, cv_res_print=False, heatmap=False):\n",
    "    \n",
    "    # split the dataset into train test\n",
    "    X_, y_ = clusters[dataset]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=42, stratify=y_)\n",
    "        \n",
    "    # params\n",
    "    param_grid = param_by_model[n_ML]\n",
    "\n",
    "    # pipeline\n",
    "    if undersample:\n",
    "        pipe = make_pipeline(\n",
    "            RandomUnderSampler(sampling_strategy='majority', random_state=42), \n",
    "            ML_algorithms[n_ML]()\n",
    "        )\n",
    "    else:\n",
    "        if n_ML in [0, 1, 3]:\n",
    "            pipe = make_pipeline(ML_algorithms[n_ML](class_weight='balanced'))\n",
    "        else:\n",
    "            pipe = make_pipeline(ML_algorithms[n_ML]())\n",
    "\n",
    "    # gridsearch\n",
    "    cv = StratifiedKFold(n_splits=n_cv, shuffle=True)\n",
    "    grid = GridSearchCV(\n",
    "        estimator = pipe,\n",
    "        param_grid = param_grid,\n",
    "        cv = cv,\n",
    "        scoring = scoring, \n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    # calculate best parameters\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # results\n",
    "    cv_results = (\n",
    "        grid.best_params_,\n",
    "        grid.cv_results_['mean_test_score'].mean(), \n",
    "        grid.cv_results_['mean_test_score'].min(), \n",
    "        grid.cv_results_['mean_test_score'].max()\n",
    "    )\n",
    "    \n",
    "    # print header\n",
    "    if undersample:\n",
    "        undersample_description = 'Undersampled'\n",
    "    else:\n",
    "        undersample_description = 'Full Dataset'\n",
    "    print(\n",
    "        '\\nScenario\\n------------------------------\\n', str(ML_algorithms[0]())[:-2], \n",
    "        dataset.title(), \n",
    "        scoring, \n",
    "        undersample_description\n",
    "    )\n",
    "    \n",
    "    if cv_res_print:\n",
    "        # print cv results\n",
    "        print('\\nCrossvalidation Results\\n------------------------------')\n",
    "        for i in cv_results:\n",
    "            print(i)\n",
    "\n",
    "    # print predictions\n",
    "    y_pred = grid.predict(X_test)\n",
    "    print('\\nClassification Report\\n------------------------------\\n', classification_report(y_test, y_pred))\n",
    "    \n",
    "    if heatmap:\n",
    "        print('\\nConfusion Matrix\\n------------------------------')\n",
    "        plt.subplots(figsize=(6, 6))\n",
    "        sns.heatmap(confusion_matrix(y_test, y_pred), vmin=0, cmap='Blues', annot=True, fmt='.0f', cbar=False,\n",
    "                   xticklabels=['Not Popular', 'Billboard Hit'], yticklabels=['Not Popular', 'Billboard Hit'])\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7164962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios to check\n",
    "\n",
    "metrics = [\n",
    "    'balanced_accuracy', 'average_precision', 'neg_brier_score', 'f1', 'f1_micro', \n",
    "    'f1_macro', 'f1_weighted', 'neg_log_loss', 'precision', 'recall', 'roc_auc', 'jaccard'\n",
    "]\n",
    "\n",
    "cluster1_keys = [\n",
    "    'cluster1_0', 'cluster1_1', 'cluster1_2', 'cluster1_3'\n",
    "]\n",
    "\n",
    "cluster2_keys = [\n",
    "    'cluster2_0', 'cluster2_1', 'cluster2_2', 'cluster2_3', 'cluster2_4', \n",
    "    'cluster2_5', 'cluster2_6', 'cluster2_7', 'cluster2_8', 'cluster2_9',\n",
    "]\n",
    "\n",
    "genre_keys = [\n",
    "    'Adult_Standard', 'Rock', 'R&B', 'Country', 'Pop', 'Rap', 'Alternative', 'EDM', 'Metal'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675a3e9",
   "metadata": {},
   "source": [
    "# Let's try this again with Spotify's Popular metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe8f12",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cc4b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.53      0.69      1986\n",
      "        True       0.01      0.79      0.02        14\n",
      "\n",
      "    accuracy                           0.54      2000\n",
      "   macro avg       0.50      0.66      0.36      2000\n",
      "weighted avg       0.99      0.54      0.69      2000\n",
      "\n",
      "Wall time: 6.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# undersampled\n",
    "fit_predict_metric_model(0, dataset='small', n_cv=5, scoring='roc_auc', cv_res_print=False, undersample=True, heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7b90263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small roc_auc Full Dataset\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.58      0.74      1986\n",
      "        True       0.01      0.64      0.02        14\n",
      "\n",
      "    accuracy                           0.58      2000\n",
      "   macro avg       0.50      0.61      0.38      2000\n",
      "weighted avg       0.99      0.58      0.73      2000\n",
      "\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# not undersampled\n",
    "fit_predict_metric_model(0, dataset='small', n_cv=5, scoring='roc_auc', cv_res_print=False, undersample=False, heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6b1734c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small balanced_accuracy Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.44      0.61      1986\n",
      "        True       0.01      0.79      0.02        14\n",
      "\n",
      "    accuracy                           0.44      2000\n",
      "   macro avg       0.50      0.61      0.32      2000\n",
      "weighted avg       0.99      0.44      0.61      2000\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small average_precision Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.54      0.70      1986\n",
      "        True       0.01      0.79      0.02        14\n",
      "\n",
      "    accuracy                           0.54      2000\n",
      "   macro avg       0.50      0.66      0.36      2000\n",
      "weighted avg       0.99      0.54      0.70      2000\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small neg_brier_score Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.41      0.58      1986\n",
      "        True       0.01      0.86      0.02        14\n",
      "\n",
      "    accuracy                           0.42      2000\n",
      "   macro avg       0.50      0.64      0.30      2000\n",
      "weighted avg       0.99      0.42      0.58      2000\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small f1 Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.59      0.74      1986\n",
      "        True       0.01      0.64      0.02        14\n",
      "\n",
      "    accuracy                           0.59      2000\n",
      "   macro avg       0.50      0.61      0.38      2000\n",
      "weighted avg       0.99      0.59      0.73      2000\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small f1_micro Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      1.00      1986\n",
      "        True       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.50      0.50      0.50      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small f1_macro Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      1.00      1986\n",
      "        True       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.50      0.50      0.50      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small f1_weighted Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      1.00      1986\n",
      "        True       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.50      0.50      0.50      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small neg_log_loss Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.36      0.53      1986\n",
      "        True       0.01      0.79      0.02        14\n",
      "\n",
      "    accuracy                           0.37      2000\n",
      "   macro avg       0.50      0.57      0.27      2000\n",
      "weighted avg       0.99      0.37      0.53      2000\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small precision Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.53      0.69      1986\n",
      "        True       0.01      0.79      0.02        14\n",
      "\n",
      "    accuracy                           0.54      2000\n",
      "   macro avg       0.50      0.66      0.36      2000\n",
      "weighted avg       0.99      0.54      0.69      2000\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small recall Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.55      0.71      1986\n",
      "        True       0.01      0.79      0.02        14\n",
      "\n",
      "    accuracy                           0.55      2000\n",
      "   macro avg       0.50      0.67      0.36      2000\n",
      "weighted avg       0.99      0.55      0.70      2000\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.53      0.69      1986\n",
      "        True       0.01      0.86      0.02        14\n",
      "\n",
      "    accuracy                           0.53      2000\n",
      "   macro avg       0.51      0.69      0.36      2000\n",
      "weighted avg       0.99      0.53      0.69      2000\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Small jaccard Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.52      0.69      1986\n",
      "        True       0.01      0.71      0.02        14\n",
      "\n",
      "    accuracy                           0.53      2000\n",
      "   macro avg       0.50      0.62      0.35      2000\n",
      "weighted avg       0.99      0.53      0.68      2000\n",
      "\n",
      "Wall time: 31.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# check metrics\n",
    "undersample = True\n",
    "\n",
    "for metric in metrics:\n",
    "    fit_predict_metric_model(0, dataset='small', n_cv=5, scoring=metric, cv_res_print=False, undersample=undersample, heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcfebebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster1_0 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.65      0.79    216792\n",
      "        True       0.01      0.68      0.01       696\n",
      "\n",
      "    accuracy                           0.65    217488\n",
      "   macro avg       0.50      0.67      0.40    217488\n",
      "weighted avg       1.00      0.65      0.78    217488\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster1_1 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.60      0.75    720840\n",
      "        True       0.02      0.66      0.04      9088\n",
      "\n",
      "    accuracy                           0.60    729928\n",
      "   macro avg       0.51      0.63      0.39    729928\n",
      "weighted avg       0.98      0.60      0.74    729928\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster1_2 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.65      0.78    416209\n",
      "        True       0.01      0.69      0.02      2585\n",
      "\n",
      "    accuracy                           0.65    418794\n",
      "   macro avg       0.50      0.67      0.40    418794\n",
      "weighted avg       0.99      0.65      0.78    418794\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster1_3 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.61      0.76    239463\n",
      "        True       0.00      0.58      0.01       504\n",
      "\n",
      "    accuracy                           0.61    239967\n",
      "   macro avg       0.50      0.60      0.38    239967\n",
      "weighted avg       1.00      0.61      0.76    239967\n",
      "\n",
      "Wall time: 2h 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# check clusters 1\n",
    "undersample = True\n",
    "\n",
    "for key in cluster1_keys:\n",
    "    fit_predict_metric_model(0, dataset=key, n_cv=5, scoring='roc_auc', cv_res_print=False, undersample=undersample, heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1e17328",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster2_0 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.58      0.73    178093\n",
      "        True       0.02      0.68      0.03      1857\n",
      "\n",
      "    accuracy                           0.58    179950\n",
      "   macro avg       0.51      0.63      0.38    179950\n",
      "weighted avg       0.98      0.58      0.73    179950\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster2_1 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.66      0.79    200284\n",
      "        True       0.01      0.69      0.02      1232\n",
      "\n",
      "    accuracy                           0.66    201516\n",
      "   macro avg       0.50      0.68      0.41    201516\n",
      "weighted avg       0.99      0.66      0.79    201516\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster2_2 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.62      0.76    141831\n",
      "        True       0.01      0.68      0.01       500\n",
      "\n",
      "    accuracy                           0.62    142331\n",
      "   macro avg       0.50      0.65      0.39    142331\n",
      "weighted avg       0.99      0.62      0.76    142331\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster2_3 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.63      0.78    137204\n",
      "        True       0.00      0.53      0.01       291\n",
      "\n",
      "    accuracy                           0.63    137495\n",
      "   macro avg       0.50      0.58      0.39    137495\n",
      "weighted avg       1.00      0.63      0.77    137495\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster2_4 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.72      0.84     74960\n",
      "        True       0.01      0.70      0.01       197\n",
      "\n",
      "    accuracy                           0.72     75157\n",
      "   macro avg       0.50      0.71      0.42     75157\n",
      "weighted avg       1.00      0.72      0.83     75157\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster2_5 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.59      0.74    166033\n",
      "        True       0.03      0.70      0.06      2967\n",
      "\n",
      "    accuracy                           0.59    169000\n",
      "   macro avg       0.51      0.65      0.40    169000\n",
      "weighted avg       0.97      0.59      0.73    169000\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster2_6 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.61      0.76     57590\n",
      "        True       0.01      0.66      0.03       475\n",
      "\n",
      "    accuracy                           0.61     58065\n",
      "   macro avg       0.50      0.63      0.39     58065\n",
      "weighted avg       0.99      0.61      0.75     58065\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster2_7 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.59      0.74    319125\n",
      "        True       0.02      0.65      0.04      3789\n",
      "\n",
      "    accuracy                           0.59    322914\n",
      "   macro avg       0.51      0.62      0.39    322914\n",
      "weighted avg       0.98      0.59      0.73    322914\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster2_8 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.63      0.77    215926\n",
      "        True       0.01      0.69      0.02      1353\n",
      "\n",
      "    accuracy                           0.63    217279\n",
      "   macro avg       0.50      0.66      0.40    217279\n",
      "weighted avg       0.99      0.63      0.77    217279\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Cluster2_9 roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.63      0.77    102259\n",
      "        True       0.00      0.58      0.01       213\n",
      "\n",
      "    accuracy                           0.63    102472\n",
      "   macro avg       0.50      0.60      0.39    102472\n",
      "weighted avg       1.00      0.63      0.77    102472\n",
      "\n",
      "Wall time: 1h 54min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# check clusters 2\n",
    "undersample = True\n",
    "\n",
    "for key in cluster2_keys:\n",
    "    fit_predict_metric_model(0, dataset=key, n_cv=5, scoring='roc_auc', cv_res_print=False, undersample=undersample, heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f7222c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Adult_Standard roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.66      0.80     41161\n",
      "        True       0.02      0.68      0.03       365\n",
      "\n",
      "    accuracy                           0.66     41526\n",
      "   macro avg       0.51      0.67      0.42     41526\n",
      "weighted avg       0.99      0.66      0.79     41526\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Rock roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.55      0.70    128938\n",
      "        True       0.03      0.71      0.05      2225\n",
      "\n",
      "    accuracy                           0.55    131163\n",
      "   macro avg       0.51      0.63      0.38    131163\n",
      "weighted avg       0.97      0.55      0.69    131163\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression R&B roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.59      0.74     26857\n",
      "        True       0.02      0.63      0.03       284\n",
      "\n",
      "    accuracy                           0.59     27141\n",
      "   macro avg       0.50      0.61      0.39     27141\n",
      "weighted avg       0.98      0.59      0.73     27141\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Country roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.65      0.79     53358\n",
      "        True       0.02      0.71      0.04       568\n",
      "\n",
      "    accuracy                           0.65     53926\n",
      "   macro avg       0.51      0.68      0.41     53926\n",
      "weighted avg       0.99      0.65      0.78     53926\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Pop roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.61      0.76    104746\n",
      "        True       0.05      0.70      0.09      3063\n",
      "\n",
      "    accuracy                           0.62    107809\n",
      "   macro avg       0.52      0.66      0.43    107809\n",
      "weighted avg       0.96      0.62      0.74    107809\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Rap roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.63      0.77     80454\n",
      "        True       0.06      0.68      0.11      2645\n",
      "\n",
      "    accuracy                           0.63     83099\n",
      "   macro avg       0.52      0.66      0.44     83099\n",
      "weighted avg       0.95      0.63      0.75     83099\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Alternative roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.59      0.74     16667\n",
      "        True       0.03      0.76      0.06       317\n",
      "\n",
      "    accuracy                           0.59     16984\n",
      "   macro avg       0.51      0.67      0.40     16984\n",
      "weighted avg       0.97      0.59      0.72     16984\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Edm roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.73      0.84     49552\n",
      "        True       0.02      0.77      0.04       362\n",
      "\n",
      "    accuracy                           0.73     49914\n",
      "   macro avg       0.51      0.75      0.44     49914\n",
      "weighted avg       0.99      0.73      0.84     49914\n",
      "\n",
      "\n",
      "Scenario\n",
      "------------------------------\n",
      " LogisticRegression Metal roc_auc Undersampled\n",
      "\n",
      "Classification Report\n",
      "------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.68      0.81     48804\n",
      "        True       0.01      0.83      0.03       270\n",
      "\n",
      "    accuracy                           0.68     49074\n",
      "   macro avg       0.51      0.75      0.42     49074\n",
      "weighted avg       0.99      0.68      0.80     49074\n",
      "\n",
      "Wall time: 36min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# check genres\n",
    "undersample = True\n",
    "\n",
    "for key in genre_keys:\n",
    "    fit_predict_metric_model(0, dataset=key, n_cv=5, scoring='roc_auc', cv_res_print=False, undersample=undersample, heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93fe68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1426b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d631ee6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb04bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c959b53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "798723c3",
   "metadata": {},
   "source": [
    "# OLDER CODE: no meaningful results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c29c2e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3793ae9b",
   "metadata": {},
   "source": [
    "#### huge loop (all night not nearly enough, better now anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5024d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for key in cluster_keys:\n",
    "    fit_predict_metric_model(0, dataset=key, n_cv=5, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c4176",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for key in cluster_keys:\n",
    "    fit_predict_metric_model(0, dataset=key, n_cv=5, scoring='average_precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d5ed7",
   "metadata": {},
   "source": [
    "#### tons of scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(0, dataset='Adult_Standard', n_cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(0, dataset='Adult_Standard', n_cv=5, scoring='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(0, dataset='Adult_Standard', n_cv=5, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db1f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(0, dataset='Adult_Standard', n_cv=5, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(0, dataset='cluster1_0', n_cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6084caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(0, dataset='cluster1_1', n_cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621076b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(0, dataset='cluster1_2', n_cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30755135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(0, dataset='cluster1_3', n_cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795b5c1",
   "metadata": {},
   "source": [
    "##### Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(1, dataset='Adult_Standard', n_cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a8e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(2, dataset='Adult_Standard', n_cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(3, dataset='Adult_Standard', n_cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_metric_model(4, dataset='Adult_Standard', n_cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3fe64",
   "metadata": {},
   "source": [
    "# EVEN OLDER CODE: no meaningful results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a602d0f",
   "metadata": {},
   "source": [
    "### Which ML models did well with default settings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd92ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('default_results.pickle', 'rb') as f:\n",
    "    default_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de167432",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_algorithms = [\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    KNeighborsClassifier,\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier\n",
    "]\n",
    "\n",
    "def default_results_by_metric(class_type='True', metric='f1-score'):\n",
    "    \"\"\"convert default results into readable form\"\"\"\n",
    "    output_ = []\n",
    "\n",
    "    for algo in ML_algorithms:\n",
    "        algo_ = str(algo())[:-2]\n",
    "        temp_ = [algo_]\n",
    "        for cluster in cluster_keys:\n",
    "            if class_type == 'accuracy':\n",
    "                metric_ = default_results[algo_][cluster][1][class_type]\n",
    "            else:\n",
    "                metric_ = default_results[algo_][cluster][1][class_type][metric]\n",
    "            temp_.append(metric_)\n",
    "        output_.append(temp_)\n",
    "\n",
    "    df_default_results = pd.DataFrame(output_, columns=['Model']+list(default_results['LogisticRegression'].keys()))\n",
    "    df_default_results['min'] = df_default_results.iloc[:, 1:].min(axis=1)\n",
    "    df_default_results['max'] = df_default_results.iloc[:, 1:].max(axis=1)\n",
    "    df_default_results['mean'] = df_default_results.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "    return df_default_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best performing classification by cluster = Adult_Standard (1st or 2nd for all ML models)\n",
    "sortbyfeature = 'LogisticRegression'\n",
    "pd.DataFrame(default_results_by_metric().iloc[:, :-3].set_index('Model').T).sort_values(sortbyfeature, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adult standard is just better because of randomness... there are more to start with ...\n",
    "for cluster in clusters:\n",
    "    print(cluster, clusters[cluster][1].sum(), clusters[cluster][1].count(), clusters[cluster][1].sum() / clusters[cluster][1].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da576a88",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huge list of orders of magnitude for gridsearch\n",
    "# formatted a bit funny to avoid rounding errors\n",
    "orders_of_magnitude = []\n",
    "for lst in [[int(x)/10000 for x in range(1, 11)],\n",
    "            [int(x)/1000 for x in range(1, 11)],\n",
    "            [int(x)/100 for x in range(1, 11)],\n",
    "            [int(x)/10 for x in range(1, 11)],\n",
    "            [1 * x for x in range(1, 11)],\n",
    "            [10 * x for x in range(1, 11)],\n",
    "            [100 * x for x in range(1, 11)],\n",
    "            [1000 * x for x in range(1, 11)]]:\n",
    "    orders_of_magnitude += lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2122121",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# LOGISTIC REGRESSION\n",
    "\n",
    "# choose dataset\n",
    "am_testing = False  # test tuning setup with small dataset\n",
    "if am_testing:\n",
    "    X_, y_ = X_small, y_small\n",
    "else:\n",
    "    X_, y_ = clusters['Adult_Standard']  # has the best classification results from STEP 5\n",
    "\n",
    "# params\n",
    "scoring = 'f1'\n",
    "param_grid = {}\n",
    "param_grid['logisticregression__penalty'] = ['l1', 'l2']\n",
    "param_grid['logisticregression__C'] = orders_of_magnitude\n",
    "\n",
    "# pipeline\n",
    "pipe = make_pipeline(\n",
    "    RandomUnderSampler(sampling_strategy='majority'), \n",
    "    LogisticRegression(solver='liblinear')\n",
    ")\n",
    "\n",
    "# gridsearch\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "lr_grid = GridSearchCV(\n",
    "    estimator = pipe,\n",
    "    param_grid = param_grid,\n",
    "    cv = cv,\n",
    "    scoring = scoring, \n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# calculate best parameters\n",
    "lr_grid.fit(X_, y_)\n",
    "\n",
    "# results\n",
    "lr_grid.best_params_, lr_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't seem like tuning is doing anything\n",
    "(\n",
    "    lr_grid.cv_results_['mean_test_score'].mean(), \n",
    "    lr_grid.cv_results_['mean_test_score'].min(), \n",
    "    lr_grid.cv_results_['mean_test_score'].max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65432a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# DECISION TREE\n",
    "\n",
    "# choose dataset\n",
    "am_testing = False  # test tuning setup with small dataset\n",
    "if am_testing:\n",
    "    X_, y_ = X_small, y_small\n",
    "else:\n",
    "    X_, y_ = clusters['Adult_Standard']  # has the best classification results from STEP 5\n",
    "\n",
    "# params\n",
    "scoring = 'f1'\n",
    "param_grid = {}\n",
    "param_grid['decisiontreeclassifier__max_depth'] = [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40, 50, 100, None]\n",
    "param_grid['decisiontreeclassifier__min_samples_leaf'] = [5, 10, 50, 100, 1000]\n",
    "param_grid['decisiontreeclassifier__criterion'] = ['gini', 'entropy']\n",
    "\n",
    "# pipeline\n",
    "pipe = make_pipeline(\n",
    "    RandomUnderSampler(sampling_strategy='majority'), \n",
    "    DecisionTreeClassifier()\n",
    ")\n",
    "\n",
    "# gridsearch\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "dt_grid = GridSearchCV(\n",
    "    estimator = pipe,\n",
    "    param_grid = param_grid,\n",
    "    cv = cv,\n",
    "    scoring = scoring, \n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# calculate best parameters\n",
    "dt_grid.fit(X_, y_)\n",
    "\n",
    "# results\n",
    "dt_grid.best_params_, dt_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44700709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning isn't useful here either...\n",
    "(\n",
    "    dt_grid.cv_results_['mean_test_score'].mean(), \n",
    "    dt_grid.cv_results_['mean_test_score'].min(), \n",
    "    dt_grid.cv_results_['mean_test_score'].max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# KNN\n",
    "\n",
    "# choose dataset\n",
    "am_testing = False  # test tuning setup with small dataset\n",
    "if am_testing:\n",
    "    X_, y_ = X_small, y_small\n",
    "else:\n",
    "    X_, y_ = clusters['Adult_Standard']  # has the best classification results from STEP 5\n",
    "\n",
    "# params\n",
    "scoring = 'f1'\n",
    "param_grid = {}\n",
    "param_grid['kneighborsclassifier__n_neighbors'] = [x for x in range(2,20)]+[x for x in range(20,101,5)]\n",
    "param_grid['kneighborsclassifier__weights'] = ['uniform', 'distance']\n",
    "param_grid['kneighborsclassifier__metric'] = ['minkowski', 'euclidean', 'manhattan']\n",
    "\n",
    "# pipeline\n",
    "pipe = make_pipeline(\n",
    "    RandomUnderSampler(sampling_strategy='majority'), \n",
    "    KNeighborsClassifier()\n",
    ")\n",
    "\n",
    "# gridsearch\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "knn_grid = GridSearchCV(\n",
    "    estimator = pipe,\n",
    "    param_grid = param_grid,\n",
    "    cv = cv,\n",
    "    scoring = scoring, \n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# calculate best parameters\n",
    "knn_grid.fit(X_, y_)\n",
    "\n",
    "# results\n",
    "knn_grid.best_params_, knn_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check min mean and max\n",
    "(\n",
    "    knn_grid.cv_results_['mean_test_score'].mean(), \n",
    "    knn_grid.cv_results_['mean_test_score'].min(), \n",
    "    knn_grid.cv_results_['mean_test_score'].max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# random forest\n",
    "\n",
    "# choose dataset\n",
    "am_testing = False  # test tuning setup with small dataset\n",
    "if am_testing:\n",
    "    X_, y_ = X_small, y_small\n",
    "else:\n",
    "    X_, y_ = clusters['Adult_Standard']  # has the best classification results from STEP 5\n",
    "\n",
    "# params\n",
    "scoring = 'f1'\n",
    "param_grid = {}\n",
    "param_grid['randomforestclassifier__n_estimators'] = [5, 10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "param_grid['randomforestclassifier__max_features'] = ['sqrt', 'log2']\n",
    "param_grid['randomforestclassifier__max_depth'] = [3, 5, 7, 10, 15, 20, 30, 50, 100, None]\n",
    "param_grid['randomforestclassifier__min_samples_leaf'] = [5, 10, 50, 100, 1000]\n",
    "param_grid['randomforestclassifier__bootstrap'] = [True, False]\n",
    "\n",
    "# pipeline\n",
    "pipe = make_pipeline(\n",
    "    RandomUnderSampler(sampling_strategy='majority'), \n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "# gridsearch\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator = pipe,\n",
    "    param_grid = param_grid,\n",
    "    cv = cv,\n",
    "    scoring = scoring, \n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# calculate best parameters\n",
    "rf_grid.fit(X_, y_)\n",
    "\n",
    "# results\n",
    "rf_grid.best_params_, rf_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also didn't do great...\n",
    "(\n",
    "    rf_grid.cv_results_['mean_test_score'].mean(), \n",
    "    rf_grid.cv_results_['mean_test_score'].min(), \n",
    "    rf_grid.cv_results_['mean_test_score'].max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# adaboost\n",
    "\n",
    "# choose dataset\n",
    "am_testing = False  # test tuning setup with small dataset\n",
    "if am_testing:\n",
    "    X_, y_ = X_small, y_small\n",
    "else:\n",
    "    X_, y_ = clusters['Adult_Standard']  # has the best classification results from STEP 5\n",
    "\n",
    "# params\n",
    "scoring = 'f1'\n",
    "param_grid = {}\n",
    "param_grid['adaboostclassifier__n_estimators'] = [10, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "param_grid['adaboostclassifier__n_learning_rate'] = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "param_grid['adaboostclassifier__n_algorithm'] = ['SAMME', 'SAMME.R']\n",
    "        \n",
    "# pipeline\n",
    "pipe = make_pipeline(\n",
    "    RandomUnderSampler(sampling_strategy='majority'), \n",
    "    AdaBoostClassifier()\n",
    ")\n",
    "\n",
    "# gridsearch\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "ab_grid = GridSearchCV(\n",
    "    estimator = pipe,\n",
    "    param_grid = param_grid,\n",
    "    cv = cv,\n",
    "    scoring = scoring, \n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# calculate best parameters\n",
    "ab_grid.fit(X_, y_)\n",
    "\n",
    "# results\n",
    "ab_grid.best_params_, ab_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b255a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check min mean and max\n",
    "(\n",
    "    ab_grid.cv_results_['mean_test_score'].mean(), \n",
    "    ab_grid.cv_results_['mean_test_score'].min(), \n",
    "    ab_grid.cv_results_['mean_test_score'].max()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ad126",
   "metadata": {},
   "source": [
    "### Test Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b7339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a65e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12613bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7548d980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8bdf80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
